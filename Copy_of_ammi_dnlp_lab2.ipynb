{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ammi_dnlp_lab2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/berthine/Deep_NLP/blob/master/Copy_of_ammi_dnlp_lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzBHf68UXD2k",
        "colab_type": "text"
      },
      "source": [
        "# Chit Chat Chatbots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm6rTDCrXJ4V",
        "colab_type": "text"
      },
      "source": [
        "In the previous lab, we explored models that try to answer questions by reasoning over free-text input. In this lab, we will explore two types of models to create chatbots.\n",
        "\n",
        "First, let's consider important qualities for a chit-chat chatbot system\n",
        "\n",
        "\n",
        "1.   **Readability** - whatever model we use, the chats it creates should be easily understood by humans\n",
        "2.   **Consistency** - when chatting with a chatbot, the bot should maintain consistent information. Imagine a bot that says \"Hi I'm Jack'' and then \"Hello, my name is Jane\" - quite confusing\n",
        "3.    **Engaging** - To encourage users to talk to the bot, the bot should be able to generate interesting, engaging responses. If the only response was \"wow, that's cool,\" users are quite unlikely to want to talk very much to the chat bot\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrYcmfoI5PWO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "1afd5e7d-d629-4711-b63d-cc9954e436a9"
      },
      "source": [
        "%%html\n",
        "<p style='color: blue;'>\n",
        "  Throughout the lab, there will be <b>questions</b> you should answer. <b>All questions you need to write an answer to will be in this blue color.</b>\n",
        "  \n",
        "  <br>Please write brief answers- no need for long explanations. \n",
        "  <br>There can be multiple correct answers to the questions.\n",
        "  \n",
        "  <br><br>The goal of these questions is to:\n",
        "  <ul style='color: green;'>\n",
        "    <li> Review the lecture material in the context of practical models and develop intuition about the models\n",
        "    <li> Develop a sense of experimentation - we will pretend we have a dataset and will walk through an experimental thought process.\n",
        "  </ul>\n",
        "\n",
        "<b>We are going to do the lab as a group. <br>I will explain the sections in more depth, as we did not cover dialogue deeply during the lecture. <br> After we discuss, I will provide time for you to write a few sentences. At the end of the lab, you will hand it in. In theory, everyone should be finished together!</b>\n",
        "\n",
        "  \n",
        "</p>"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style='color: blue;'>\n",
              "  Throughout the lab, there will be <b>questions</b> you should answer. <b>All questions you need to write an answer to will be in this blue color.</b>\n",
              "  \n",
              "  <br>Please write brief answers- no need for long explanations. \n",
              "  <br>There can be multiple correct answers to the questions.\n",
              "  \n",
              "  <br><br>The goal of these questions is to:\n",
              "  <ul style='color: green;'>\n",
              "    <li> Review the lecture material in the context of practical models and develop intuition about the models\n",
              "    <li> Develop a sense of experimentation - we will pretend we have a dataset and will walk through an experimental thought process.\n",
              "  </ul>\n",
              "\n",
              "<b>We are going to do the lab as a group. <br>I will explain the sections in more depth, as we did not cover dialogue deeply during the lecture. <br> After we discuss, I will provide time for you to write a few sentences. At the end of the lab, you will hand it in. In theory, everyone should be finished together!</b>\n",
              "\n",
              "  \n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3zg0WczXjDs",
        "colab_type": "text"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kguDIJbLXmZU",
        "colab_type": "text"
      },
      "source": [
        "The dataset we will use for this lab is called `PersonaChat` - it was created to directly address problem 2. Each person talking in the dataset has a personality, which helps maintain consistency in the dialogue. We saw it last week in the tutorials as well (when you worked through beam search)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpvTb8bBX40A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        },
        "outputId": "ff89df4c-599b-4d6a-8ae7-d27a4c6d5578"
      },
      "source": [
        "!git clone https://github.com/facebookresearch/ParlAI.git ~/ParlAI  > /dev/null\n",
        "!cd ~/ParlAI; python setup.py develop > /dev/null"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/root/ParlAI'...\n",
            "remote: Enumerating objects: 92, done.\u001b[K\n",
            "remote: Counting objects: 100% (92/92), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 29746 (delta 47), reused 43 (delta 19), pack-reused 29654\u001b[K\n",
            "Receiving objects: 100% (29746/29746), 57.90 MiB | 25.09 MiB/s, done.\n",
            "Resolving deltas: 100% (21144/21144), done.\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "/usr/local/lib/python3.6/dist-packages/setuptools/dist.py:454: UserWarning: Normalizing '2019.08.19' to '2019.8.19'\n",
            "  warnings.warn(tmpl.format(**locals()))\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kfolded_char_at\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:10625:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kfolded_len\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kfolded_len\u001b[m\u001b[K;\n",
            "         \u001b[01;35m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kfuzzy_match_group_fld\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:11503:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kdata.new_text_pos\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
            "     if (!\u001b[01;35m\u001b[Krecord_fuzzy(state, data.fuzzy_type, data.new_text_pos - data.step)\u001b[m\u001b[K)\n",
            "          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kfuzzy_match_string_fld\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:11270:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kdata.new_text_pos\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
            "     if (!\u001b[01;35m\u001b[Krecord_fuzzy(state, data.fuzzy_type, data.new_text_pos - data.step)\u001b[m\u001b[K)\n",
            "          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbasic_match\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:11608:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kdata.new_text_pos\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
            "     if (!\u001b[01;35m\u001b[Krecord_fuzzy(state, data.fuzzy_type, data.new_text_pos - data.step)\u001b[m\u001b[K)\n",
            "          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:11522:18:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[Kdata.new_text_pos\u001b[m\u001b[K’ was declared here\n",
            "     RE_FuzzyData \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K;\n",
            "                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:11369:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kdata.new_text_pos\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
            "     if (!\u001b[01;35m\u001b[Krecord_fuzzy(state, data.fuzzy_type, data.new_text_pos - data.step)\u001b[m\u001b[K)\n",
            "          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:11288:18:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[Kdata.new_text_pos\u001b[m\u001b[K’ was declared here\n",
            "     RE_FuzzyData \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K;\n",
            "                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "regex.__pycache__._regex.cpython-36: module references __file__\n",
            "In file included from \u001b[01m\u001b[Kext/_yaml.c:591:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kext/_yaml.h:2:10:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kyaml.h: No such file or directory\n",
            " #include \u001b[01;31m\u001b[K<yaml.h>\u001b[m\u001b[K\n",
            "          \u001b[01;31m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "compilation terminated.\n",
            "Error compiling module, falling back to pure Python\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "warning: no files found matching 'README.txt'\n",
            "warning: no files found matching 'Makefile' under directory '*.txt'\n",
            "warning: no previously-included files matching '*~' found anywhere in distribution\n",
            "warning: no previously-included files found matching '.pre-commit-hooks.yaml'\n",
            "warning: no previously-included files found matching '.travis.yml'\n",
            "warning: no previously-included files found matching 'Makefile'\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "warning: no previously-included files found matching '.travis.yml'\n",
            "warning: no previously-included files found matching 'Makefile'\n",
            "warning: no previously-included files found matching 'test_acid.py'\n",
            "zip_safe flag not set; analyzing archive contents...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OJJ-L5RCyYT",
        "colab_type": "text"
      },
      "source": [
        "**Example: **\n",
        "\n",
        "your persona: i just started college.\n",
        "\n",
        "your persona: i have 3 science classes.\n",
        "\n",
        "your persona: i work part time in the campus library.\n",
        "\n",
        "your persona: i am living at home but hope to live in the dorms next year.\n",
        "\n",
        "**Partner Dialogue**: hi how are you doing\n",
        "\n",
        "**Your Response**: great ! just got off work and relaxing before i study"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe72XiqFXCS0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "fba8bb22-048e-4e35-a7e3-73bb0884ecdf"
      },
      "source": [
        "# let's download and take a look at some examples of data in PersonaChat\n",
        "!python ~/ParlAI/examples/display_data.py --task personachat --datatype train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[creating task(s): personachat]\n",
            "[building data: /root/ParlAI/data/Persona-Chat]\n",
            "[ downloading: http://parl.ai/downloads/personachat/personachat.tgz to /root/ParlAI/data/Persona-Chat/personachat.tgz ]\n",
            "Downloading personachat.tgz: 100% 223M/223M [00:06<00:00, 36.4MB/s]\n",
            "[ Checksum Successful ]\n",
            "unpacking personachat.tgz\n",
            "[loading fbdialog data:/root/ParlAI/data/Persona-Chat/personachat/train_self_original.txt]\n",
            "\u001b[1;31m- - - NEW EPISODE: personachat - - -\u001b[0;0m\n",
            "\u001b[0myour persona: i like to remodel homes.\n",
            "your persona: i like to go hunting.\n",
            "your persona: i like to shoot a bow.\n",
            "your persona: my favorite holiday is halloween.\n",
            "hi , how are you doing ? i am getting ready to do some cheetah chasing to stay in shape .\u001b[0;0m\n",
            "   \u001b[1;94myou must be very fast . hunting is one of my favorite hobbies .\u001b[0;0m\n",
            "\u001b[0mi am ! for my hobby i like to do canning or some whittling .\u001b[0;0m\n",
            "   \u001b[1;94mi also remodel homes when i am not out bow hunting .\u001b[0;0m\n",
            "\u001b[0mthat is neat . when i was in high school i placed 6th in 100m dash !\u001b[0;0m\n",
            "   \u001b[1;94mthat is awesome . do you have a favorite season or time of year ?\u001b[0;0m\n",
            "\u001b[0mi do not . but i do have a favorite meat since that is all i eat exclusively .\u001b[0;0m\n",
            "   \u001b[1;94mwhat is your favorite meat to eat ?\u001b[0;0m\n",
            "\u001b[0mi would have to say its prime rib . do you have any favorite foods ?\u001b[0;0m\n",
            "   \u001b[1;94mi like chicken or macaroni and cheese .\u001b[0;0m\n",
            "\u001b[0mdo you have anything planned for today ? i think i am going to do some canning .\u001b[0;0m\n",
            "   \u001b[1;94mi am going to watch football . what are you canning ?\u001b[0;0m\n",
            "\u001b[0mi think i will can some jam . do you also play footfall for fun ?\u001b[0;0m\n",
            "   \u001b[1;94mif i have time outside of hunting and remodeling homes . which is not much !\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: personachat - - -\u001b[0;0m\n",
            "\u001b[0myour persona: my mom is my best friend.\n",
            "your persona: i have four sisters.\n",
            "your persona: i believe that mermaids are real.\n",
            "your persona: i love iced tea.\n",
            "hi , how are you doing today ?\u001b[0;0m\n",
            "   \u001b[1;94mi am spending time with my 4 sisters what are you up to\u001b[0;0m\n",
            "\u001b[0mwow , four sisters . just watching game of thrones .\u001b[0;0m\n",
            "   \u001b[1;94mthat is a good show i watch that while drinking iced tea\u001b[0;0m\n",
            "\u001b[0mi agree . what do you do for a living ?\u001b[0;0m\n",
            "   \u001b[1;94mi am a researcher i am researching the fact that mermaids are real\u001b[0;0m\n",
            "[ loaded 8939 episodes with a total of 65719 examples ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Ob4tEt1vNP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "3950f757-cfbf-4e15-8bc1-8235132a3cdc"
      },
      "source": [
        "%%html\n",
        "<p style='color: blue;'>\n",
        "  <b>Questions:</b>\n",
        "  <ul style='color: blue;'>\n",
        "    <li>What do the personalities look like?</li>\n",
        "    <li>How does creating bots with these simple personalities address consistency for chatbots? </li>\n",
        "    <li>What are some drawbacks/limitations of these specific personalities for addressing the problem of consistency?</li>\n",
        "  </ul>\n",
        "</p>"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style='color: blue;'>\n",
              "  <b>Questions:</b>\n",
              "  <ul style='color: blue;'>\n",
              "    <li>What do the personalities look like?</li>\n",
              "    <li>How does creating bots with these simple personalities address consistency for chatbots? </li>\n",
              "    <li>What are some drawbacks/limitations of these specific personalities for addressing the problem of consistency?</li>\n",
              "  </ul>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaOrbyVgWnlU",
        "colab_type": "text"
      },
      "source": [
        "**Answers**\n",
        "\n",
        "**- What do the personalities look like?**\n",
        "\n",
        "The personnalities are looking simple, interesting  and clear. \n",
        "\n",
        " **- How does creating bots with these simple personalities address consistency for chatbots?**\n",
        "\n",
        " With these simple personalities the domain of conversation with  is limited. The chatbot can not go out. For example if the personalities are about footbal you can not ask a question related to finance.\n",
        "\n",
        " **- What are some drawbacks/limitations of these specific personalities for addressing the problem of consistency?**\n",
        "\n",
        "The drawbacks/limitations of these specific personalities:\n",
        "\n",
        "- We can not ask general questions, it only contains limited personalities.\n",
        "- It too focus on the domain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kyhri6NvYKda",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "***Let's understand how much data we have. Let's compute the following using ParlAI:***\n",
        "\n",
        "\n",
        "1.   **How many turns of data do we have?** In dialogue datasets, \"amount of data\" is measured in dialogue turns. Each time there is a single line of dialogue, that is called a \"turn\"\n",
        "2.   **On average, how many words form a model input?**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30SF6hN6DAjD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "outputId": "bdca310c-1fcc-4928-d4a3-54c34741a981"
      },
      "source": [
        "!python ~/ParlAI/parlai/scripts/data_stats.py -t personachat -dt train -ltim 10000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ note: changing datatype from train to train:ordered ]\n",
            "[creating task(s): personachat]\n",
            "[loading fbdialog data:/root/ParlAI/data/Persona-Chat/personachat/train_self_original.txt]\n",
            "[ loaded 8939 episodes with a total of 65719 examples ]\n",
            "16s elapsed:\n",
            "      %done  \\\n",
            "    100.00%   \n",
            "      exs  \\\n",
            "    65719   \n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                 stats  \\\n",
            "    \n",
            "input:\n",
            "   utterances: 65719\n",
            "   avg utterance length: 18.356974390967604\n",
            "   tokens: 1206402\n",
            "   unique tokens: 14209\n",
            "   unique utterances: 64580\n",
            "labels:\n",
            "   utterances: 65719\n",
            "   avg utterance length: 11.929411585690591\n",
            "   tokens: 783989\n",
            "   unique tokens: 14507\n",
            "   unique utterances: 64119\n",
            "both:\n",
            "   utterances: 131438\n",
            "   avg utterance length: 15.143192988329098\n",
            "   tokens: 1990391\n",
            "   unique tokens: 18741\n",
            "   unique utterances: 128197\n",
            "   \n",
            "   time_left  \n",
            "          0s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vwk7krxU57M",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "How are dialogue models evaluated?\n",
        "\n",
        "\n",
        "\n",
        "1.   **Automatic Evaluation**: Hits @ 1, Hits @ 5, Hits @ 10, F1\n",
        "2.   **Human Evaluation**: Pairing Selection, Human Rating\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGI2fFPLVRSJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "283cb271-fea5-415b-a4f3-402fad7282bf"
      },
      "source": [
        "%%html\n",
        "<p style='color: blue;'>\n",
        "  <b>Questions:</b>\n",
        "  <ul style='color: blue;'>\n",
        "    <li>Take some notes about what these metrics are and what they mean here</li>\n",
        "  </ul>\n",
        "</p>"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style='color: blue;'>\n",
              "  <b>Questions:</b>\n",
              "  <ul style='color: blue;'>\n",
              "    <li>Take some notes about what these metrics are and what they mean here</li>\n",
              "  </ul>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXL46eskdtgT",
        "colab_type": "text"
      },
      "source": [
        "**- Evaluation** is about to understand and know when your model is made  improvevement.\n",
        "\n",
        "**- Automatic Evaluation** basically means run a script and get an answer immediatelly. \n",
        "\n",
        "**- Hits @k**: means that if you have a set of retrieval candidates for examples k= 100, for  my model to say hit @k means the rate of correct utterance appearing in the top k entries for each instance list. \n",
        "It's very useful for retrieval based dialogue models.\n",
        "\n",
        "**- Hits @1** means that when the model has scored all the candidates and the highest ranked one is the correct utterance.\n",
        "\n",
        "**- Hits @5** means that the correct answers are ranked in the top 5.\n",
        "\n",
        "**- Hits @10** means that the correct answers are ranked in the top 10.\n",
        "\n",
        "**- F1** : is a mix of precision and recall. It can be use to evaluate generative and retrieval models. It takes the human text and split into individual words and takes also what the model has written, plits into terms of words and calcutalte the precision and recall.\n",
        "\n",
        "**- Human Evaluation** basically need a human to read and decide how good  is the model. \n",
        "\n",
        "**- Pairing selection** in pairing we compare two items.\n",
        "**- Human ranking**  when humans are asked to rate the model based on several criteria such as task completion success, comprehension. etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpkwidfAZ_7p",
        "colab_type": "text"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuqnjqxbZ_W2",
        "colab_type": "text"
      },
      "source": [
        "There are two main kinds of dialogue models. \n",
        "\n",
        "**Retrieval** Models analyze the current dialogue context and try to find appropriate responses in the dataset.\n",
        "\n",
        "**Generative** Models analyze the current dialogue context\n",
        "and try to write an answer, word by word, from left to right.\n",
        "This can be thought of as an application of sequence-to-sequence models,  where the \"encoder side\" is the dialogue history and the \"decoder side\" is the dialogue response your chatbot should generate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPRp1MlH3n0h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "3a1dfc1f-8ac5-4b10-a06e-0e2ce1b592c5"
      },
      "source": [
        "%%html\n",
        "<p style='color: blue;'>\n",
        "  <b>Questions:</b>\n",
        "  <ul style='color: blue;'>\n",
        "    <li>Let's discuss the pros/cons of retrieval compared to generative models - Are there settings when you might want to use one over the other?</li>\n",
        "    <li>Compare a chit-chat application to something like booking a movie ticket- would you want to use generative, retrieval, or something else to accomplish that task? Why?</li>\n",
        "    <li>How can you evaluate generative models with the metrics we discussed before? How do you think they will perform compared to retrieval models?</li>\n",
        "    <li>In lecture, Antoine mentioned issues with generative model generation being generic and short. How does this happen in beam search?</li>\n",
        "</ul>\n",
        "\n",
        "<b> If you would like me to discuss how to actually use generative models in dialog, please say something! Otherwise, we will skip.</b>\n",
        "</p>"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style='color: blue;'>\n",
              "  <b>Questions:</b>\n",
              "  <ul style='color: blue;'>\n",
              "    <li>Let's discuss the pros/cons of retrieval compared to generative models - Are there settings when you might want to use one over the other?</li>\n",
              "    <li>Compare a chit-chat application to something like booking a movie ticket- would you want to use generative, retrieval, or something else to accomplish that task? Why?</li>\n",
              "    <li>How can you evaluate generative models with the metrics we discussed before? How do you think they will perform compared to retrieval models?</li>\n",
              "    <li>In lecture, Antoine mentioned issues with generative model generation being generic and short. How does this happen in beam search?</li>\n",
              "</ul>\n",
              "\n",
              "<b> If you would like me to discuss how to actually use generative models in dialog, please say something! Otherwise, we will skip.</b>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnD1wkB4Hizj",
        "colab_type": "text"
      },
      "source": [
        "**Answers**\n",
        "\n",
        "**- Let's discuss the pros/cons of retrieval compared to generative models**\n",
        "\n",
        "**Retrieval models** the benefit is that if you retrieve you don't have to make  mistakes if your entire training set is basically built of things that human said. You won't make mistakes in grammar.\n",
        "\n",
        "**Generative models**  can generate new text, they're  not as restricted to the training set if they completely have not seen terms of words. They're able to adapt to a new situation, the capacity to generate responses even  if they do not figure in the corpus. They are quite likely to make grammatical mistake especially for long sentences.\n",
        "\n",
        "**- Are there settings when you might want to use one over the other?**\n",
        "\n",
        "- Generative models require huge amount of training data otherwise use the retrieval models which doesn't require a huge training data..\n",
        "- In Retrieval models the candidate responses should be limited if not the model will run forever. \n",
        "\n",
        "**- Compare a chit-chat application to something like booking a movie ticket- would you want to use generative, retrieval, or something else to accomplish that task? Why?**\n",
        "\n",
        "Booking a movie ticket, the generative or retrieval models will no be able to accomplish the tasks. Because these  can not accomplish completly (100%)  the task, we need something else what  is specific, not expensive to accomplish  perfectly the task.\n",
        "\n",
        "**- How can you evaluate generative models with the metrics we discussed before? How do you think they will perform compared to retrieval models?**\n",
        "\n",
        "Hits @ is the evaluation metric used for retrieval models and F1 can be use for both retrieval and generative models.To perform  generally, it  depends on the specific quality of the model. For example you train the model using a basketball dataset and test using the soccer dataset,  the generative models will perform better than the retrieval models. \n",
        "\n",
        "**- In lecture, Antoine mentioned issues with generative model generation being generic and short. How does this happen in beam search?**\n",
        "\n",
        "When generative models are put into situation where they don't know what to say, they often say something super generic and short for example \"Hello! how are you?\", \"I don't know\". \n",
        "\n",
        "In beam search, every step of the beam search, the model try to predict and depending of size of the beam probable next candidates and then it selects like the top performing beam size candidates to continue in the beam search.\n",
        "\n",
        "Since beam search works by maximizing the probability over the sentence, if something new happen, it will be penalize by the beam search.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_99CoAwaW2w",
        "colab_type": "text"
      },
      "source": [
        "### Retrieval Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAfk4Km8bY_Y",
        "colab_type": "text"
      },
      "source": [
        "Let's train a model to do retrieval first. We will try the *Memory Net.* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyNOObOkbYTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can train a model with the following command: \n",
        "# !python ~/ParlAI/examples/train_model.py -m kv_memnn -t personachat -dt train -veps 0.25 --model-file persona_chat_retrieval_model -vmt accuracy\n",
        "\n",
        "# but we have limited time in the tutorial, so let's use an already pretrained model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gylhJvZwa0Vp",
        "colab_type": "text"
      },
      "source": [
        "Quick Parameter Refresher:\n",
        "\n",
        "\n",
        "*  `-m ` means which model we're going to use. Recall retrieval models are trained to rank the true response higher over a set of potential responses from the dataset (in ParlAI, these are called the \"label candidates\"). When it's time to write a dialogue response, the retrieval model returns the response that is ranked the highest\n",
        "*  -`t` refers to the task. Here, we are training on PersonaChat data.\n",
        "* `-dt` refers to the data split. We want to train our model, so we are using the training set.\n",
        "* `-veps` refers to how often we should evaluate during training, our performance on validation. recall this is important because models, particularly neural ones, have the capacity to memorize the training dataset. So it's important to check how the model is doing on the validation set.\n",
        "* `--model-file` refers to when your model is saved, what should the filename be\n",
        "*  `-vmt` refers to the metric which we'll use to decide which model is the best. We'll cover this in the next section\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6wdfxEccAAp",
        "colab_type": "text"
      },
      "source": [
        "**Let's interact with the model to get a sense of what it's learning. **How is this chat going to work?\n",
        "\n",
        "\n",
        "\n",
        "1.   You will be assigned a persona. You will chat to the model by typing in the chat box.\n",
        "2.   The chatbot also has a persona. It's secret and hidden from you!\n",
        "3.   When you've finished chatting with this bot, type [DONE] and a new model persona will be assigned to the bot, so you can talk to a new bot. \n",
        "4.   When you move on to the next chatbot persona, the previous persona will be revealed. \n",
        "\n",
        "Interact with the chatbots and the personas. **Try to think about the following:**\n",
        "\n",
        "*   Do the chatbots follow their persona a lot?\n",
        "*   Was it difficult to follow your persona?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdyaEgpcI7UL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "525ded54-e4ac-4356-93b4-619c6819a69d"
      },
      "source": [
        "!python ~/ParlAI/projects/convai2/interactive.py -mf models:convai2/kvmemnn/model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[building data: /root/ParlAI/data/models/convai2/kvmemnn/kvmemnn.tgz]\n",
            "[ downloading: http://parl.ai/downloads/_models/convai2/kvmemnn.tgz to /root/ParlAI/data/models/convai2/kvmemnn/kvmemnn.tgz ]\n",
            "Downloading kvmemnn.tgz: 100% 173M/173M [00:05<00:00, 32.6MB/s]\n",
            "unpacking kvmemnn.tgz\n",
            "[ warning: overriding opt['model_file'] to /root/ParlAI/data/models/convai2/kvmemnn/model (previously: /checkpoint/jase/20180328/kvmemnn_sweep10/persona-self_rephraseTrn-True_rephraseTst-False_lr-0.1_esz-2000_margin-0.1_tfidf-False_shareEmb-True_hops1_lins0/model )]\n",
            "[ creating KvmemnnAgent ]\n",
            "Dictionary: loading dictionary from /root/ParlAI/data/models/convai2/kvmemnn/model.dict\n",
            "[ num words =  19153 ]\n",
            "Loading existing model params from /root/ParlAI/data/models/convai2/kvmemnn/model\n",
            "[loading candidates: /root/ParlAI/data/models/convai2/kvmemnn/model.candspair*]\n",
            "[caching..]\n",
            "=init done=\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "[creating task(s): parlai.agents.local_human.local_human:LocalHumanAgent]\n",
            "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "[ optional arguments: ] \n",
            "[  display_examples: False ]\n",
            "[  display_ignore_fields: label_candidates,text_candidates ]\n",
            "[  display_prettify: False ]\n",
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 1 ]\n",
            "[  datapath: /root/ParlAI/data ]\n",
            "[  datatype: train ]\n",
            "[  download_path: /private/home/jase/src/ParlAI/downloads ]\n",
            "[  dynamic_batching: None ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  init_opt: None ]\n",
            "[  multitask_weights: [1] ]\n",
            "[  numthreads: 40 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: convai2:SelfRevisedTeacher ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
            "[  init_model: None ]\n",
            "[  model: projects.personachat.kvmemnn.kvmemnn:KvmemnnAgent ]\n",
            "[  model_file: /root/ParlAI/data/models/convai2/kvmemnn/model ]\n",
            "[ Local Human Arguments: ] \n",
            "[  local_human_candidates_file: None ]\n",
            "[  single_turn: False ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "[ Dictionary Arguments: ] \n",
            "[  bpe_debug: False ]\n",
            "[  dict_endtoken: __END__ ]\n",
            "[  dict_file: /root/ParlAI/data/models/convai2/kvmemnn/model.dict ]\n",
            "[  dict_initpath: None ]\n",
            "[  dict_language: english ]\n",
            "[  dict_lower: False ]\n",
            "[  dict_max_ngram_size: -1 ]\n",
            "[  dict_maxtokens: -1 ]\n",
            "[  dict_minfreq: 0 ]\n",
            "[  dict_nulltoken: __NULL__ ]\n",
            "[  dict_starttoken: __START__ ]\n",
            "[  dict_textfields: text,labels ]\n",
            "[  dict_tokenizer: split ]\n",
            "[  dict_unktoken: __UNK__ ]\n",
            "[ BPEHelper Arguments: ] \n",
            "[  bpe_add_prefix_space: None ]\n",
            "[  bpe_merge: None ]\n",
            "[  bpe_vocab: None ]\n",
            "[ Kvmemnn Arguments: ] \n",
            "[  cache_size: 1000 ]\n",
            "[  embeddingnorm: 10 ]\n",
            "[  embeddingsize: 2000 ]\n",
            "[  history_length: 100 ]\n",
            "[  history_replies: label ]\n",
            "[  hops: 1 ]\n",
            "[  interactive_mode: False ]\n",
            "[  kvmemnn_debug: False ]\n",
            "[  learningrate: 0.1 ]\n",
            "[  lins: 0 ]\n",
            "[  loadcands: True ]\n",
            "[  loss: cosine ]\n",
            "[  margin: 0.1 ]\n",
            "[  neg_samples: 10 ]\n",
            "[  optimizer: sgd ]\n",
            "[  parrot_neg: 0 ]\n",
            "[  share_embeddings: True ]\n",
            "[  take_next_utt: False ]\n",
            "[  tfidf: False ]\n",
            "[  truncate: -1 ]\n",
            "[  twohop_blend: 0 ]\n",
            "[  twohop_range: 100 ]\n",
            "[creating task(s): convai2:both]\n",
            "[building data: /root/ParlAI/data/ConvAI2]\n",
            "[ downloading: http://parl.ai/downloads/convai2/convai2_fix_723.tgz to /root/ParlAI/data/ConvAI2/convai2_fix_723.tgz ]\n",
            "Downloading convai2_fix_723.tgz: 100% 430M/430M [00:08<00:00, 50.2MB/s]\n",
            "[ Checksum Successful ]\n",
            "unpacking convai2_fix_723.tgz\n",
            "[loading fbdialog data:/root/ParlAI/data/ConvAI2/train_both_original.txt]\n",
            "your persona: my favorite is rock fish.\n",
            "your persona: i m going riding on my boat today.\n",
            "your persona: i love tuna.\n",
            "your persona: i live in cape hatteras.\n",
            "your persona: i'm a fisherman.\n",
            "Enter [DONE] if you want a new partner at any time.\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m hi\n",
            "\u001b[0;34m[[no id field]]:\u001b[0;0m \u001b[1mhey there . so , have any hobbies ? i enjoy playing video games .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m how are you\n",
            "\u001b[0;34m[[no id field]]:\u001b[0;0m \u001b[1mi am good dog . what about you ? what are you into ?\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m dog?\n",
            "\u001b[0;34m[[no id field]]:\u001b[0;0m \u001b[1mit is a golden retriever puppy ! my last dog died when i was 18 .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m sorry\n",
            "\u001b[0;34m[[no id field]]:\u001b[0;0m \u001b[1msorry to hear that . it must have been so hard\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m done\n",
            "\u001b[0;34m[[no id field]]:\u001b[0;0m \u001b[1mmy mother works as a ballet dancer . she gets her nails done too .\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m Traceback (most recent call last):\n",
            "  File \"/root/ParlAI/projects/convai2/interactive.py\", line 127, in <module>\n",
            "    interactive(parser.parse_args(print_args=False), print_parser=parser)\n",
            "  File \"/root/ParlAI/projects/convai2/interactive.py\", line 102, in interactive\n",
            "    acts[0] = agents[0].act()\n",
            "  File \"/root/ParlAI/parlai/agents/local_human/local_human.py\", line 67, in act\n",
            "    reply_text = input(colorize(\"Enter Your Message:\", 'text') + ' ')\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNbf3YEp31Z0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "6007cf86-a3e9-48ec-a543-4a39b0db0fb6"
      },
      "source": [
        "%%html\n",
        "<p style='color: blue;'>\n",
        "  <b>Questions:</b>\n",
        "  <ul style='color: blue;'>\n",
        "    <li>What does this model seem to be doing well? What is it doing poorly? </li>\n",
        "    <li>Why might it be performing poorly? What kind of experiment could you design to test your hypothesis?</li>\n",
        "    <li>How do we know if we need to use a more complex model? Would we always want to use a more complex model? Why or why not?</li>\n",
        "  </ul>\n",
        "</p>"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style='color: blue;'>\n",
              "  <b>Questions:</b>\n",
              "  <ul style='color: blue;'>\n",
              "    <li>What does this model seem to be doing well? What is it doing poorly? </li>\n",
              "    <li>Why might it be performing poorly? What kind of experiment could you design to test your hypothesis?</li>\n",
              "    <li>How do we know if we need to use a more complex model? Would we always want to use a more complex model? Why or why not?</li>\n",
              "  </ul>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1_tbE5uV4IA",
        "colab_type": "text"
      },
      "source": [
        "**Answers**\n",
        "\n",
        "**What does this model seem to be doing well? What is it doing poorly?** \n",
        "\n",
        "What the model does well is that when you type a message that the model knows is giving a good answer otherwise is trying to give an answer  even if it not related to what you are talking about.\n",
        "\n",
        "What it doing poorly is that sometimes is giving wrong answer not related to your message.\n",
        "\n",
        "\n",
        "**Why might it be performing poorly? What kind of experiment could you design to test your hypothesis?**\n",
        "\n",
        "The A/B testing can be used to improve  a chatbot, to  optimize for better results.\n",
        "\n",
        "So before I begin creating your chatbot experiments, I will first choose the metrics that I want to improve.\n",
        "\n",
        "For example, if am using a chatbot for marketing, my metric could be the number of leads who opt-in after a successful chatbot interaction.\n",
        "\n",
        "Alternatively, if am using a chatbot for boosting sales, my metric could be the number of trial leads whose engagement score improves because of interacting with the chatbot.\n",
        "\n",
        "Whatever it is, once I’ve identified the metric  to optimize, I can start  working on the chatbot experiment.\n",
        "\n",
        "A typical A/B test involves three steps:\n",
        "\n",
        "  - The problem. The issue I want to solve\n",
        "  - The control. The existing feature or experience I am testing against, without which, I wouldn’t know whether my experiment was successful\n",
        "\n",
        "- The proposed solution. The new approach I’ve come up with to solve the problem my hypothesis.\n",
        "\n",
        "**How do we know if we need to use a more complex model? Would we always want to use a more complex model? Why or why not?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_SYFM2UaUuq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here is a command to train a Transformer Ranker model if you would like to try it out\n",
        "# !python ~/ParlAI/examples/train_model.py -m transformer/ranker -t personachat -dt train -veps 0.25 --model-file persona_chat_retrieval_model -vmt accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd7Sfurfdc_3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "An important aspect of training models is analyzing them. ***Try to answer the following questions.*** \n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1pAkQdt4n5_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "c3c9f03a-4187-41c1-d0c7-626b66557a27"
      },
      "source": [
        "%%html\n",
        "<p style='color: blue;'>\n",
        "  <b>Questions:</b>\n",
        "  <ul style='color: blue;'>\n",
        "    <li>Are the models using the persona that we have provided? How can you tell? If I asked you to prove it to me, what experiments could you conduct? </li>\n",
        "    <li>Previously, we computed some statistics about how long the persona is in the training data. The model has also only seen words present in the training dataset. But what happens if you push the model outside of what data it's been trained on? What kind of performance do you get? Why does this happen, and what could you do if you wanted to improve the model's ability to generalize? </li>\n",
        "    <li>In ParlAI, we've set the parameters to save the model's best performance based on validation accuracy. What would happen if we saved the model based on the best training accuracy? Why does this happen? (if you like, try this out on your own and see the effect when you interact with the bot)</li>\n",
        "  </ul>\n",
        "\n",
        "<b> If you would like me to discuss how to use BERT in dialog, please say something! Otherwise, we will skip.</b>\n",
        "</p>"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style='color: blue;'>\n",
              "  <b>Questions:</b>\n",
              "  <ul style='color: blue;'>\n",
              "    <li>Are the models using the persona that we have provided? How can you tell? If I asked you to prove it to me, what experiments could you conduct? </li>\n",
              "    <li>Previously, we computed some statistics about how long the persona is in the training data. The model has also only seen words present in the training dataset. But what happens if you push the model outside of what data it's been trained on? What kind of performance do you get? Why does this happen, and what could you do if you wanted to improve the model's ability to generalize? </li>\n",
              "    <li>In ParlAI, we've set the parameters to save the model's best performance based on validation accuracy. What would happen if we saved the model based on the best training accuracy? Why does this happen? (if you like, try this out on your own and see the effect when you interact with the bot)</li>\n",
              "  </ul>\n",
              "\n",
              "<b> If you would like me to discuss how to use BERT in dialog, please say something! Otherwise, we will skip.</b>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTHItY-QdNtw",
        "colab_type": "text"
      },
      "source": [
        "### [for self exploration] Generative Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM6LaBzbe6Za",
        "colab_type": "text"
      },
      "source": [
        "Generative models must produce word for word what they are going to say next in the dialogue. When predicting the next word, it produces a probability distribution over the entire vocabulary space for which word to generate next. To reduce the vocabulary space, we will use **byte-pair encoding** (BPE). \n",
        "\n",
        "*How does BPE work?* The BPE algorithm takes as input the training data and the number of *operations* it can do. It passes over the training set and tries to create sub-word units. For example, the word \"beautiful\" might be split into \"beau\" \"ti\" \"ful\". Each time it splits a word into sub-words, that is one operation. The final vocabulary output consists of these subwords. So \"ful\" can be part of \"beautiful\" and part of \"fruitful\" and so on.\n",
        "\n",
        "\n",
        "**Questions to ask yourself**:\n",
        "\n",
        "\n",
        "1.   Why is it important to keep the vocabulary space small?\n",
        "2.   What does perplexity measure? Why would we use it as a training objective? \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4gNbbAugACx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !python ~/ParlAI/examples/train_model.py -m transformer/generator -t personachat -dt train -veps 0.25 --model-file persona_chat_generative_model -vmt ppl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHpD0Ccb1zhd",
        "colab_type": "text"
      },
      "source": [
        "## Final Thoughts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHdsZ3Sq12n6",
        "colab_type": "text"
      },
      "source": [
        "**What did we learn about dialogue modeling? Review Questions to ask yourself**\n",
        "\n",
        "*   How do retrieval models work? What about generative? What are their pros and cons?\n",
        "*   What are some important traits of dialogue systems? How might the traits differ for different dialogue tasks?\n",
        "\n",
        "\n",
        "**General Takeaways about Machine Learning and Experimentation:**\n",
        "\n",
        "*   We don't try models just to try them - try to have a reason for conducting an experiment. As we did in the lab, try to analyze what's working well in your models and working poorly. Try to use these reasons to guide why you might want to try other models. Complex is not necessarily better. \n",
        "*   Certain models can be better for certain tasks. As we've seen, generative models are working really well for tasks such as machine translation, but have a bit to go before becoming general purpose dialogue generators. \n",
        "\n",
        "\n",
        "\n",
        "**I'm really interested in dialogue! What can I do to learn more?**\n",
        "\n",
        "\n",
        "*   Play around in ParlAI: ParlAI is a general library with many great dialogue models and code for them. It also provides a standard interface to access datasets and interact with various models. \n",
        "*   Read the PersonaChat Paper: https://arxiv.org/pdf/1801.07243.pdf\n",
        "*   Dialog using knowledge: One challenge of these chit chat systems is they do not concretely know any facts. So if you want to chat about a specific topic, the models cannot produce any relevant information - they say generic utterances or incorrect facts. One way to remedy this is to incorporate **knowledge** into the dialogue agents. This has been investigated in many different ways, but one of the first papers to show this is https://arxiv.org/abs/1811.01241. In this work, data is collected by asking one speaker to reference Wikipedia sentences.\n",
        "* Dialog with BERT: pretty new,  there is an investigation of two ways to use BERT in this paper: https://arxiv.org/abs/1903.03094. \n",
        "\n",
        "\n"
      ]
    }
  ]
}